import time
import json
import requests
from tqdm import tqdm
import os

os.makedirs("data/processed", exist_ok=True)


def load_json(path):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return None


def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def fetch_full_resumes(token, raw_resumes):
    import logging

    logging.basicConfig(
        level=logging.INFO,
        filename="data/processed/fetch_full.log",
        filemode="a",
        format="%(asctime)s %(levelname)s:%(message)s"
    )

    headers = {
        "Authorization": f"Bearer {token}",
        "User-Agent": "ai-resume-screener/1.0"
    }

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã–µ –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—é–º–µ
    full_path = "data/processed/resumes_full.json"
    raw_path = "data/raw/resumes_raw.json"
    full_resumes = load_json(full_path) or []

    # –°–æ–∑–¥–∞—ë–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ ID —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö —Ä–µ–∑—é–º–µ
    downloaded_ids = {r.get("id") for r in full_resumes if "id" in r}

    print(f"üìå –£–∂–µ —Å–∫–∞—á–∞–Ω–æ –ø–æ–ª–Ω—ã—Ö —Ä–µ–∑—é–º–µ: {len(downloaded_ids)}")

    wait_seconds_error = 60    # —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø—Ä–∏ 429
    wait_seconds = 300    # —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø—Ä–∏ 429
    max_retries = 5
    save_every = 1      # —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ

    new_count = 0

    for idx, short_res in enumerate(tqdm(raw_resumes, desc="üì• –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö —Ä–µ–∑—é–º–µ")):
        rid = short_res.get("id")
        if not rid:
            continue

        # –ï—Å–ª–∏ —Ä–µ–∑—é–º–µ —É–∂–µ —Å–∫–∞—á–∞–Ω–æ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if rid in downloaded_ids:
            continue

        url = f"https://api.hh.ru/resumes/{rid}"
        retry = 0

        while retry < max_retries:
            try:
                resp = requests.get(url, headers=headers, timeout=10)
            except requests.exceptions.RequestException as e:
                logging.warning(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {rid}: {e}")
                retry += 1
                time.sleep(wait_seconds_error)
                continue

            if resp.status_code == 200:
                full_resumes.append(resp.json())
                downloaded_ids.add(rid)
                new_count += 1
                logging.info(f"‚úÖ –°–∫–∞—á–∞–Ω–æ —Ä–µ–∑—é–º–µ {rid}")
                break

            elif resp.status_code == 404:
                logging.warning(f"‚ö† 404 ‚Äî —Ä–µ–∑—é–º–µ {rid} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                raw_resumes.remove(short_res)
                save_json(raw_path, raw_resumes)
                break

            elif resp.status_code == 429:
                logging.warning(
                    f"‚ö† 429 (–ª–∏–º–∏—Ç). –†–µ–∑—é–º–µ {rid}. –ü–æ–≤—Ç–æ—Ä {retry + 1}/{max_retries}. "
                    f"–ñ–¥—ë–º {wait_seconds} —Å–µ–∫—É–Ω–¥."
                )
                retry += 1
                time.sleep(wait_seconds)

            else:
                logging.warning(f"‚ö† –û—à–∏–±–∫–∞ {resp.status_code} –¥–ª—è {rid}: {resp.text}")
                break

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        if new_count % save_every == 0 and new_count > 0:
            save_json(full_path, full_resumes)

        time.sleep(0.5)

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    save_json(full_path, full_resumes)

    print(f"üì¶ –ù–æ–≤—ã—Ö –ø–æ–ª–Ω—ã—Ö —Ä–µ–∑—é–º–µ —Å–∫–∞—á–∞–Ω–æ: {new_count}")
    print(f"üì¶ –í—Å–µ–≥–æ —Ç–µ–ø–µ—Ä—å –≤ –±–∞–∑–µ: {len(full_resumes)}")

    return full_resumes
